
# ğŸ¤– Robot Arm RL Project Guide

Welcome to the **Robot Arm Reinforcement Learning** project!  
This guide will help you set up, understand, and run everything without breaking your laptop (hopefully).  

---

## ğŸ“‚ Project Structure
```

RobotArm/
â”‚â”€â”€ envs/
â”‚   â””â”€â”€ robot\_arm\_env.py      # Custom MuJoCo Gymnasium environment
â”‚
â”‚â”€â”€ lowCostRobotArm/
â”‚   â”œâ”€â”€ robotScene.xml        # Scene definition (floor, cube, goal)
â”‚   â””â”€â”€ low\_cost\_robot\_arm.xml# Robot arm model
â”‚
â”‚â”€â”€ train\_ppo.py              # Script to train PPO agent
â”‚â”€â”€ eval\_policy.py             # Script to evaluate trained agent
â”‚â”€â”€ requirements.txt           # Dependencies
â”‚â”€â”€ README.md                  # This guide
â”‚
â”œâ”€â”€ models/ppo\_robot\_arm/      # Trained models (.zip files)
â”œâ”€â”€ logs/ppo\_robot\_arm/        # Training logs & monitor.csv
â””â”€â”€ eval/ppo\_robot\_arm/        # Evaluation results

````

---

## âš™ï¸ Setup Instructions

### 1. Clone the Repo
```bash
git clone <repo-link>
cd RobotArm
````

If you donâ€™t use GitHub: unzip the shared folder and `cd` into it.

---

### 2. Create Virtual Environment

```bash
python -m venv venv
venv\Scripts\activate    # on Windows
source venv/bin/activate # on Linux/Mac
```

---

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

Key dependencies:

* `gymnasium`
* `mujoco`
* `stable-baselines3`
* `torch`
* `numpy`

---

## ğŸš€ Running the Project

### 1. Train the Agent

```bash
python train_ppo.py
```

* Model checkpoints will be saved in `models/ppo_robot_arm/`.
* Training logs go into `logs/ppo_robot_arm/monitor.csv`.
* **Note:** training takes hours. Donâ€™t expect miracles immediately.

---

### 2. Evaluate the Trained Agent

```bash
python eval_policy.py
```

* Opens a MuJoCo viewer where you see the arm attempt to grab the cube.
* Prints episode rewards in the terminal.

---

## ğŸ“Š Files Explained

* **`robot_arm_env.py`** â†’ Defines how the robot arm interacts with the environment:

  * Action space = joint controls.
  * Observation = gripper + cube + goal + joint states.
  * Reward = closer to cube + closer to goal + bonus for grasp.

* **`train_ppo.py`** â†’ Trains PPO with Stable-Baselines3:

  * Uses `EvalCallback` (saves best model).
  * Uses `CheckpointCallback` (saves periodic snapshots).

* **`eval_policy.py`** â†’ Loads trained model and runs a few test episodes.

* **`.xml files`** â†’ Define robot, cube, and goal positions.

---

## ğŸ’¾ Sharing Models

Since trained models are large `.zip` files:

* Stored in `models/ppo_robot_arm/`.
* If not in repo, check Google Drive link (to be shared separately).
* To test someone elseâ€™s model, place the `.zip` inside this folder and update `MODEL_NAME` in `eval_policy.py`.

---

## ğŸ§© Common Issues

* **No viewer opens?**
  â†’ Ensure you installed MuJoCo properly and your GPU/CPU supports rendering.

* **Error: Gym unmaintained**
  â†’ Weâ€™re already using `gymnasium`, so double-check you didnâ€™t install old `gym`.

* **No movement during eval?**
  â†’ Might be an untrained/random model. Wait for longer training or use a provided trained model.

---

## ğŸ“Œ Workflow Tips

* **Donâ€™t retrain from scratch unless necessary.** Use provided checkpoints.
* **Use virtual environments** to avoid dependency hell.
* **Check logs** (`logs/ppo_robot_arm/monitor.csv`) for episode reward trends.
